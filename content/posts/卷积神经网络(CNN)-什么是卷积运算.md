---
Date: '2021-08-05'
Draft: false
Categories: ['Tech']
Title: WordPress 博客搭建
---
# 卷积计算的来源

想象这么一个例子，有一些图片，我们要判断这些图片中含不含有猫。如果用基本的神经网络做二分类（logistics regression）的话，当以一张`64x64x3`的图片作为输入进行神经网络运算时，其输入端总共有12288个输入，当神经网络只有一层，且这一层有1000个节点时，那么对于这个神经网络，连接输入与神经网络节点之间的的权值矩阵将是一个（1000，12288）的矩阵，也就是有`12,288,000`个参数需要确定，也就是1000万左右，这还仅仅是一张1.5k左右的图片，进行训练的时候就需要确定1000万个参数，如果图片再大一点，`1000*1000*3`的话（366K），那么需要确定的参数将达到3亿个。这么多的参数，就会带来两个问题：

- 神经网络训练困难
- 容易出现过拟合

为了解决这种类型的问题，不得不寻找新的计算方法来应对计算机视觉中的图像处理问题，也就有了**卷积运算**。  
![常规神经网络的参数量](https://gitee.com/agcl/oss/raw/master/img/20210805155718.png)

# 卷积运算(2维卷积为例)

## 卷积定义(Convolution)

在泛函分析中，**卷积、旋积或褶积(英语：Convolution)**是通过两个函数f和g生成第三个函数的一种数学算子，表征函数f与g经过翻转和平移的重叠部分函数值乘积对重叠长度的积分。 如果将参加卷积的一个函数看作区间的指示函数，卷积还可以被看作是“滑动平均”的推广。卷积是一种特殊的线性操作。卷积网络是一种特殊的神经网络，它们在至少一个层中使用卷积代替一般矩阵乘法。

## 卷积核与过滤矩阵(kernel & Filter)

卷积核就是**权重矩阵**，只不过是确定大小的权重矩阵。多个卷积核线性连接在一起，也就变成了过滤矩阵，一般来说，一个二维的卷积核，线性相连在一起就变成了一个三维的矩阵，也就是说过滤矩阵的3D的，所以，Filter总是要比Kernel多一层。卷积核的作用就是从图像中提取出某些特征，比如提出图片的横向边缘，纵向边缘，或者任意角度的边缘检测。

[卷积核和过滤矩阵科普博客](https://zhuanlan.zhihu.com/p/87763131)

与基本的神经网络不同，卷积运算可以说已经指定了运算法则，也就是制定了权重矩阵，而来训练一个**过滤器矩阵**，完成卷积运算。

## 进行卷积运算

卷积运算就是将原始的输入矩阵与卷积核按照一定的规则相乘，规则如下：

1. 将卷积核与输入矩阵一一匹配，并做相乘相加运算，放到生成矩阵的对应位置

![](https://gitee.com/agcl/oss/raw/master/img/20210805162731.png)

比如说上图，从左上角开始，将整个filter与前面的6x6的矩阵对应上，对应位置的数做相乘运算，然后相加起来，放到第一个位置。也就是  
3*1+1*2+2*1+0*0+0*5+0*7+1*(-1)+8*(-1)+2*(-1)=-5  
填入第一个空缺。

2. 将卷积核往右移动一个单位，继续做相同的运算，将运算得到的结果填入第二个空缺

![](https://gitee.com/agcl/oss/raw/master/img/20210805163222.png)

3. 依次向后移动，当移动到最后石，回过头来往下移动一个格子，从左到右依次的做**卷积运算**

![](https://gitee.com/agcl/oss/raw/master/img/20210805163414.png)

4. 直到后面的4x4的矩阵完全填满，那么一次卷积运算也就做完了。

![](https://gitee.com/agcl/oss/raw/master/img/20210805163551.png)

## 生成矩阵的维度确定

在输入矩阵和卷积核都确定的情况下，最终卷积的结果也是确定的，结果的维度为 输入矩阵维度n-过滤矩阵维度f + 1  
n2 = n1 - f + 1  
如上面的输出矩阵的维度为 6 -3 + 1 = 4

# Padding

在进行卷积运算时，每次卷积过后，整个矩阵会变小，久而久之**整个矩阵会变得很小**。而且位于左上角的数字3只参与了一次卷积运算，而位于中间的数字2则进行了多次卷积运算，也就是影响多个卷积结果。这样会让图片的边缘信息得不到充分的挖掘与利用，于是就有了在输入矩阵周围拓展一些空白的像素点，使得边缘的信息能够充分利用。

![](https://gitee.com/agcl/oss/raw/master/img/20210805172104.png)

上图中蓝色的部分就是添加的padding，这样可以保证，边缘的信息能够得到充分的利用，而且输出的矩阵的大小可以得到控制。更进一步，**可以通过控制padding的大小，使得输入矩阵和输入矩阵的大小维度是一致的。**

令：n+2p - f + 1 = n  
解得: p = (f-1)/2  
如上图p=1，则有 6+2*1-3+1=6

根据padding大小的不同，可以将卷积分为**Valid Convolution** 和**Same Convolution**两大类，显而易见，前者就叫空白卷积，不添加任何padding，后者叫相同卷积，也就是保证卷积后的矩阵大小与卷积前的矩阵大小保持一致。

# 卷积步长（Strided Convolution）

在前面的例子中，每次都是向右移动一个单位，而步长就是用来控制移动几个单位的。比如步长为2，那么在进行卷积运算时，当第一个矩阵部分运算完毕，就向右移动两格子，进行下一次运算。

![](https://gitee.com/agcl/oss/raw/master/img/20210805173644.png)

![](https://gitee.com/agcl/oss/raw/master/img/20210805173715.png)

此时输出矩阵的维度可以用下面的公式计算![](https://gitee.com/agcl/oss/raw/master/img/20210805173917.png)

# 总结

卷积计算主要是利用简单的线性计算，将一个较大的输入矩阵，转化成一个小一点的矩阵，其中的计算很简单。但是上面的卷积运算都是基于二维的输入矩阵和二维的卷积核，最终得到一个二维的输出矩阵。那为什么卷积运算比基本的神经网络运算的效果更好呢？对于3D矩阵的卷积应该怎么计算呢？下一节将介绍的内容为：

1. 卷积为什么有效？
2. 多通道图像(3维输入矩阵)的卷积怎么计算？
3. 单层卷积网络

[amoxil online](https://buyantibiotics24.net/buy-amoxil-online.html)